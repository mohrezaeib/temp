{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How to build an accurate sentiment analysis model with handful training examples\n",
    "\n",
    "- Suppose we want to train a model to classify product data into 2 categories: positive and negative.\n",
    "- We use a single model for each type of product (or domain).\n",
    "- However, in some domains, we only have a limited number of training examples (low-resource domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\mohre\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\mohre\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\mohre\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\mohre\\anaconda3\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mohre\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>EC</th>\n",
       "      <th>EnzName</th>\n",
       "      <th>OptpH</th>\n",
       "      <th>Org</th>\n",
       "      <th>Acc</th>\n",
       "      <th>ref</th>\n",
       "      <th>seq</th>\n",
       "      <th>seq_char_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.1.1.37</td>\n",
       "      <td>malate dehydrogenase</td>\n",
       "      <td>7.5</td>\n",
       "      <td>Mesembryanthemum crystallinum</td>\n",
       "      <td>O24047</td>\n",
       "      <td>656923</td>\n",
       "      <td>MAVEPLRVLVTGAAGQIGYALVPMIARGIMLGANQPVILHMLDIPP...</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.1.1.37</td>\n",
       "      <td>malate dehydrogenase</td>\n",
       "      <td>7.5</td>\n",
       "      <td>Moritella sp.</td>\n",
       "      <td>Q7X3X5</td>\n",
       "      <td>655635</td>\n",
       "      <td>MKVAVLGAAGGIGQALALLLKTQLPAGSELSLYDIAPVTPGVAVDL...</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.1.1.37</td>\n",
       "      <td>malate dehydrogenase</td>\n",
       "      <td>7.5</td>\n",
       "      <td>Rasamsonia emersonii</td>\n",
       "      <td>Q8TG27</td>\n",
       "      <td>655486</td>\n",
       "      <td>MFATRQAFNLFQKRAFSASARQSSKVAILGAAGGIGQPLSLLMKLN...</td>\n",
       "      <td>339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.1.1.37</td>\n",
       "      <td>malate dehydrogenase</td>\n",
       "      <td>7.5</td>\n",
       "      <td>Rasamsonia emersonii</td>\n",
       "      <td>Q8X1C8</td>\n",
       "      <td>655486</td>\n",
       "      <td>MFATRQAFNLFQKRAFSASARQSSKVAILGAAGGIGQPLSLLMKLN...</td>\n",
       "      <td>339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.1.1.37</td>\n",
       "      <td>malate dehydrogenase</td>\n",
       "      <td>7.6</td>\n",
       "      <td>Archaeoglobus fulgidus</td>\n",
       "      <td>O08349</td>\n",
       "      <td>721445</td>\n",
       "      <td>MKLGFVGAGRVGSTSAFTCLLNLDVDEIALVDIAEDLAVGEAMDLA...</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.1.1.37</td>\n",
       "      <td>malate dehydrogenase</td>\n",
       "      <td>7.6</td>\n",
       "      <td>Taenia solium</td>\n",
       "      <td>F1C7I4</td>\n",
       "      <td>722201</td>\n",
       "      <td>MPGPLRVLITGAAGQIAYNLSNMVANGNLFGKDQKIILHLLDIPEA...</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1.1.1.37</td>\n",
       "      <td>malate dehydrogenase</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Methylomicrobium alcaliphilum</td>\n",
       "      <td>G4T3S9</td>\n",
       "      <td>740975</td>\n",
       "      <td>MKTPVKIAVTGAAGQISYSLLFRLASGELLGPDQPMIFHLLETPQA...</td>\n",
       "      <td>327</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1.1.1.37</td>\n",
       "      <td>malate dehydrogenase</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Pyrobaculum calidifontis</td>\n",
       "      <td>A3MWU9</td>\n",
       "      <td>733816</td>\n",
       "      <td>MITIVGSGRVGTAAAAIMGIMRIDKKILLIDIVKGLPQGEALDLNH...</td>\n",
       "      <td>309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.1.1.37</td>\n",
       "      <td>malate dehydrogenase</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Thermus thermophilus</td>\n",
       "      <td>P10584</td>\n",
       "      <td>741286</td>\n",
       "      <td>MKAPVRVAVTGAAGQIGYSLLFRIAAGEMLGKDQPVILQLLEIPQA...</td>\n",
       "      <td>327</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.2.1.19</td>\n",
       "      <td>aminobutyraldehyde dehydrogenase</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Zea mays</td>\n",
       "      <td>C6KEM4</td>\n",
       "      <td>742854</td>\n",
       "      <td>MAPPQTIPRRGLFIGGAWREPCLGRRLPVVNPATEATIGDIPAGTA...</td>\n",
       "      <td>506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1        EC                           EnzName  \\\n",
       "0           5             5  1.1.1.37              malate dehydrogenase   \n",
       "1           6             6  1.1.1.37              malate dehydrogenase   \n",
       "2           7             7  1.1.1.37              malate dehydrogenase   \n",
       "3           8             8  1.1.1.37              malate dehydrogenase   \n",
       "4           9             9  1.1.1.37              malate dehydrogenase   \n",
       "5          10            10  1.1.1.37              malate dehydrogenase   \n",
       "6          11            11  1.1.1.37              malate dehydrogenase   \n",
       "7          12            12  1.1.1.37              malate dehydrogenase   \n",
       "8          13            13  1.1.1.37              malate dehydrogenase   \n",
       "9           5             5  1.2.1.19  aminobutyraldehyde dehydrogenase   \n",
       "\n",
       "   OptpH                            Org     Acc     ref  \\\n",
       "0    7.5  Mesembryanthemum crystallinum  O24047  656923   \n",
       "1    7.5                  Moritella sp.  Q7X3X5  655635   \n",
       "2    7.5           Rasamsonia emersonii  Q8TG27  655486   \n",
       "3    7.5           Rasamsonia emersonii  Q8X1C8  655486   \n",
       "4    7.6         Archaeoglobus fulgidus  O08349  721445   \n",
       "5    7.6                  Taenia solium  F1C7I4  722201   \n",
       "6   10.0  Methylomicrobium alcaliphilum  G4T3S9  740975   \n",
       "7   10.0       Pyrobaculum calidifontis  A3MWU9  733816   \n",
       "8   10.0           Thermus thermophilus  P10584  741286   \n",
       "9    9.4                       Zea mays  C6KEM4  742854   \n",
       "\n",
       "                                                 seq  seq_char_count  label  \n",
       "0  MAVEPLRVLVTGAAGQIGYALVPMIARGIMLGANQPVILHMLDIPP...             332      0  \n",
       "1  MKVAVLGAAGGIGQALALLLKTQLPAGSELSLYDIAPVTPGVAVDL...             312      0  \n",
       "2  MFATRQAFNLFQKRAFSASARQSSKVAILGAAGGIGQPLSLLMKLN...             339      0  \n",
       "3  MFATRQAFNLFQKRAFSASARQSSKVAILGAAGGIGQPLSLLMKLN...             339      0  \n",
       "4  MKLGFVGAGRVGSTSAFTCLLNLDVDEIALVDIAEDLAVGEAMDLA...             294      0  \n",
       "5  MPGPLRVLITGAAGQIAYNLSNMVANGNLFGKDQKIILHLLDIPEA...             332      0  \n",
       "6  MKTPVKIAVTGAAGQISYSLLFRLASGELLGPDQPMIFHLLETPQA...             327      1  \n",
       "7  MITIVGSGRVGTAAAAIMGIMRIDKKILLIDIVKGLPQGEALDLNH...             309      1  \n",
       "8  MKAPVRVAVTGAAGQIGYSLLFRIAAGEMLGKDQPVILQLLEIPQA...             327      1  \n",
       "9  MAPPQTIPRRGLFIGGAWREPCLGRRLPVVNPATEATIGDIPAGTA...             506      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let inspect the data\n",
    "# import json\n",
    "from random import shuffle\n",
    "# data = json.load(open('dataset.json'))\n",
    "\n",
    "# data[:5]\n",
    "\n",
    "import pandas as pd \n",
    "import os\n",
    "DATA_PATH = \"../maml-bert/data/raw/ph.csv\"\n",
    "data = pd.read_csv(DATA_PATH, index_col=None)\n",
    "data.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1.1.1.37': 9,\n",
       "         '1.2.1.19': 6,\n",
       "         '1.1.1.195': 19,\n",
       "         '1.13.11.B6': 5,\n",
       "         '1.1.1.25': 15,\n",
       "         '1.1.1.42': 7,\n",
       "         '1.3.2.3': 3,\n",
       "         '1.1.1.28': 3,\n",
       "         '1.11.1.21': 1,\n",
       "         '1.2.1.44': 5,\n",
       "         '2.3.1.97': 3,\n",
       "         '1.2.1.12': 12,\n",
       "         '2.4.1.14': 8,\n",
       "         '1.8.1.4': 3,\n",
       "         '1.1.99.18': 6,\n",
       "         '1.2.1.9': 6,\n",
       "         '1.3.1.42': 5,\n",
       "         '1.1.1.17': 1,\n",
       "         '1.4.1.2': 1,\n",
       "         '1.1.1.183': 1,\n",
       "         '1.2.1.8': 6,\n",
       "         '1.6.2.4': 3,\n",
       "         '1.18.1.2': 2,\n",
       "         '2.3.1.180': 5,\n",
       "         '1.1.1.40': 14,\n",
       "         '1.4.3.2': 1,\n",
       "         '1.11.1.6': 13,\n",
       "         '1.8.5.1': 4,\n",
       "         '1.1.1.307': 1,\n",
       "         '1.1.2.8': 3,\n",
       "         '2.4.1.120': 1,\n",
       "         '1.1.1.34': 1,\n",
       "         '1.1.1.1': 16,\n",
       "         '2.3.1.177': 1,\n",
       "         '1.1.1.244': 1,\n",
       "         '2.3.1.5': 5,\n",
       "         '1.5.1.3': 8,\n",
       "         '1.1.1.219': 11,\n",
       "         '1.1.1.205': 8,\n",
       "         '1.1.1.79': 1,\n",
       "         '1.1.1.184': 5,\n",
       "         '1.11.1.15': 2,\n",
       "         '1.13.11.39': 3,\n",
       "         '1.13.11.52': 2,\n",
       "         '2.4.1.10': 4,\n",
       "         '1.14.18.1': 12,\n",
       "         '2.3.2.24': 16,\n",
       "         '1.1.3.6': 4,\n",
       "         '1.14.14.18': 6,\n",
       "         '2.3.1.196': 1,\n",
       "         '1.11.1.7': 2,\n",
       "         '2.1.2.1': 3,\n",
       "         '1.1.1.2': 3,\n",
       "         '1.12.1.2': 1,\n",
       "         '1.5.99.12': 1,\n",
       "         '1.13.11.20': 1,\n",
       "         '1.16.1.1': 1,\n",
       "         '1.15.1.1': 34,\n",
       "         '1.1.1.10': 2,\n",
       "         '1.1.1.27': 6,\n",
       "         '1.10.3.2': 35,\n",
       "         '1.4.1.4': 2,\n",
       "         '1.13.11.12': 2,\n",
       "         '2.3.1.51': 3,\n",
       "         '1.14.99.1': 3,\n",
       "         '2.3.1.30': 8,\n",
       "         '2.4.1.13': 15,\n",
       "         '1.5.5.2': 7,\n",
       "         '2.4.1.15': 4,\n",
       "         '1.1.1.41': 1,\n",
       "         '1.13.11.58': 3,\n",
       "         '1.7.1.6': 4,\n",
       "         '2.2.1.6': 11,\n",
       "         '1.11.1.19': 2})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "ec_number = [r for r in data['EC'].values]\n",
    "Counter(ec_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to the statistics, we have 100 training examples for \"office_products\", \"automotive\", \"computer_&_video_games\"\n",
    "Can we still build an accurate model on these domain ? --> Absolutetly\n",
    "#### Solution: We leverage data from high-resource domains to create a good \"starting point\". And from this point, we start training a specific model for low-resource domain \n",
    " - Approach #1: Transfer learning.: We train a single model (Model_X) on concatenate data from high-resource domains. Then, we retrain Model_X on low-resource domain\n",
    "\n",
    "\n",
    " - Approach #2: Meta learning: We stimulate a lot of situations where the Model_X are forced to learn fast with only few training examples. The model_X are getting better at \"learning with less\" after each training situation. We called these situations as Meta-task. Each task contain two sets:\n",
    "   - Support set: contain few training samples\n",
    "   - Query set: Provide learning feedback. The model use this feedback to adapt its learning strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let create meta learning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import json, pickle\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "LABEL_MAP  = {'positive':0, 'negative':1, 0:'positive', 1:'negative'}\n",
    "\n",
    "class MetaTask(Dataset):\n",
    "    \n",
    "    def __init__(self, examples, num_task, k_support, k_query, tokenizer):\n",
    "        \"\"\"\n",
    "        :param samples: list of samples\n",
    "        :param num_task: number of training tasks.\n",
    "        :param k_support: number of support sample per task\n",
    "        :param k_query: number of query sample per task\n",
    "        \"\"\"\n",
    "        self.examples = examples\n",
    "        random.shuffle(self.examples)\n",
    "        \n",
    "        self.num_task = num_task\n",
    "        self.k_support = k_support\n",
    "        self.k_query = k_query\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = 512\n",
    "        self.create_batch(self.num_task)\n",
    "    \n",
    "    def create_batch(self, num_task):\n",
    "        self.supports = []  # support set\n",
    "        self.queries = []  # query set\n",
    "        \n",
    "        for b in range(num_task):  # for each task\n",
    "            # 1.select domain randomly\n",
    "            #domain = random.choice(self.examples)['domain']\n",
    "            #domainExamples = [e for e in self.examples if e['domain'] == domain]\n",
    "            \n",
    "            # 1.select k_support + k_query examples from domain randomly\n",
    "            selected_examples = random.sample(self.examples,self.k_support + self.k_query)\n",
    "            random.shuffle(selected_examples)\n",
    "            exam_train = selected_examples[:self.k_support]\n",
    "            exam_test  = selected_examples[self.k_support:]\n",
    "            \n",
    "            self.supports.append(exam_train)\n",
    "            self.queries.append(exam_test)\n",
    "\n",
    "    def create_feature_set(self,examples):\n",
    "        all_input_ids      = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_attention_mask = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_segment_ids    = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_label_ids      = torch.empty(len(examples), dtype = torch.long)\n",
    "\n",
    "        for id_,example in enumerate(examples):\n",
    "            input_ids = tokenizer.encode(example['seq'])\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            segment_ids    = [0] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < self.max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                attention_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            label_id = LABEL_MAP[example['label']]\n",
    "            all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
    "            all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
    "            all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
    "            all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
    "\n",
    "        tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)  \n",
    "        return tensor_set\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        support_set = self.create_feature_set(self.supports[index])\n",
    "        query_set   = self.create_feature_set(self.queries[index])\n",
    "        return support_set, query_set\n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return self.num_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split meta training and meta testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 11\n"
     ]
    }
   ],
   "source": [
    "#low_resource_domains = [\"office_products\", \"automotive\", \"computer_&_video_games\"]\n",
    "train_examples = [r for r in data ]\n",
    "test_examples = [r for r in data ]\n",
    "print(len(train_examples), len(test_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated git hooks.\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/Rostlab/prot_bert_bfd_localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Model name 'Rostlab/prot_bert_bfd_localization' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed 'Rostlab/prot_bert_bfd_localization' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mohre\\anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mresolved_config_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mohre\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# File, but it doesn't exist.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"file {} not found\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: file Rostlab/prot_bert_bfd_localization not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4804/1423023669.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd_localization\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Rostlab/prot_bert_bfd_localization\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# from transformers import BertModel, BertTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mohre\\anaconda3\\lib\\site-packages\\transformers\\modeling_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mRobertaForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;34m'bert'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;34m'xlnet'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mXLNetForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mohre\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;31m# Load config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[0;32m    284\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mohre\\anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m                         \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained_config_archive_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                         config_file, CONFIG_NAME)\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mconfig_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Model name 'Rostlab/prot_bert_bfd_localization' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed 'Rostlab/prot_bert_bfd_localization' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd_localization\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Rostlab/prot_bert_bfd_localization\")\n",
    "\n",
    "# from transformers import BertModel, BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert_bfd_localization', do_lower_case = False)\n",
    "train = MetaTask(train_examples, num_task = 100, k_support=100, k_query=30, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a glance at the first two samples from support set of 1st meta-task\n",
    "train.supports[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information of the 1st meta-task. It contains two TensorDataset: support set and query set\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let take a look at the first two samples from support set\n",
    "train[0][0][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "def random_seed(value):\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    np.random.seed(value)\n",
    "    random.seed(value)\n",
    "\n",
    "def create_batch_of_tasks(taskset, is_shuffle = True, batch_size = 4):\n",
    "    idxs = list(range(0,len(taskset)))\n",
    "    if is_shuffle:\n",
    "        random.shuffle(idxs)\n",
    "    for i in range(0,len(idxs), batch_size):\n",
    "        yield [taskset[idxs[i]] for i in range(i, min(i + batch_size,len(taskset)))]\n",
    "\n",
    "class TrainingArgs:\n",
    "    def __init__(self):\n",
    "        self.num_labels = 2\n",
    "        self.meta_epoch=10\n",
    "        self.k_spt=80\n",
    "        self.k_qry=20\n",
    "        self.outer_batch_size = 2\n",
    "        self.inner_batch_size = 12\n",
    "        self.outer_update_lr = 5e-5\n",
    "        self.inner_update_lr = 5e-5\n",
    "        self.inner_update_step = 10\n",
    "        self.inner_update_step_eval = 40\n",
    "        self.bert_model = 'Rostlab/prot_bert_bfd_localization'\n",
    "        self.num_task_train = 500\n",
    "        self.num_task_test = 5\n",
    "\n",
    "args = TrainingArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Meta Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertForSequenceClassification\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Learner(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "        \n",
    "        self.num_labels = args.num_labels\n",
    "        self.outer_batch_size = args.outer_batch_size\n",
    "        self.inner_batch_size = args.inner_batch_size\n",
    "        self.outer_update_lr  = args.outer_update_lr\n",
    "        self.inner_update_lr  = args.inner_update_lr\n",
    "        self.inner_update_step = args.inner_update_step\n",
    "        self.inner_update_step_eval = args.inner_update_step_eval\n",
    "        self.bert_model = args.bert_model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.model = BertForSequenceClassification.from_pretrained(self.bert_model, num_labels = self.num_labels)\n",
    "        self.outer_optimizer = Adam(self.model.parameters(), lr=self.outer_update_lr)\n",
    "        self.model.train()\n",
    "\n",
    "    def forward(self, batch_tasks, training = True):\n",
    "        \"\"\"\n",
    "        batch = [(support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset)]\n",
    "        \n",
    "        # support = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
    "        \"\"\"\n",
    "        task_accs = []\n",
    "        sum_gradients = []\n",
    "        num_task = len(batch_tasks)\n",
    "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
    "\n",
    "        for task_id, task in enumerate(batch_tasks):\n",
    "            support = task[0]\n",
    "            query   = task[1]\n",
    "            \n",
    "            fast_model = deepcopy(self.model)\n",
    "            fast_model.to(self.device)\n",
    "            support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
    "                                            batch_size=self.inner_batch_size)\n",
    "            \n",
    "            inner_optimizer = Adam(fast_model.parameters(), lr=self.inner_update_lr)\n",
    "            fast_model.train()\n",
    "            \n",
    "            print('----Task',task_id, '----')\n",
    "            for i in range(0,num_inner_update_step):\n",
    "                all_loss = []\n",
    "                for inner_step, batch in enumerate(support_dataloader):\n",
    "                    \n",
    "                    batch = tuple(t.to(self.device) for t in batch)\n",
    "                    input_ids, attention_mask, segment_ids, label_id = batch\n",
    "                    outputs = fast_model(input_ids, attention_mask, segment_ids, labels = label_id)\n",
    "                    \n",
    "                    loss = outputs[0]              \n",
    "                    loss.backward()\n",
    "                    inner_optimizer.step()\n",
    "                    inner_optimizer.zero_grad()\n",
    "                    \n",
    "                    all_loss.append(loss.item())\n",
    "                \n",
    "                if i % 4 == 0:\n",
    "                    print(\"Inner Loss: \", np.mean(all_loss))\n",
    "            \n",
    "            fast_model.to(torch.device('cpu'))\n",
    "            \n",
    "            if training:\n",
    "                meta_weights = list(self.model.parameters())\n",
    "                fast_weights = list(fast_model.parameters())\n",
    "\n",
    "                gradients = []\n",
    "                for i, (meta_params, fast_params) in enumerate(zip(meta_weights, fast_weights)):\n",
    "                    gradient = meta_params - fast_params\n",
    "                    if task_id == 0:\n",
    "                        sum_gradients.append(gradient)\n",
    "                    else:\n",
    "                        sum_gradients[i] += gradient\n",
    "\n",
    "            fast_model.to(self.device)\n",
    "            fast_model.eval()\n",
    "            with torch.no_grad():\n",
    "                query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
    "                query_batch = iter(query_dataloader).next()\n",
    "                query_batch = tuple(t.to(self.device) for t in query_batch)\n",
    "                q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch\n",
    "                q_outputs = fast_model(q_input_ids, q_attention_mask, q_segment_ids, labels = q_label_id)\n",
    "\n",
    "                q_logits = F.softmax(q_outputs[1],dim=1)\n",
    "                pre_label_id = torch.argmax(q_logits,dim=1)\n",
    "                pre_label_id = pre_label_id.detach().cpu().numpy().tolist()\n",
    "                q_label_id = q_label_id.detach().cpu().numpy().tolist()\n",
    "\n",
    "                acc = accuracy_score(pre_label_id,q_label_id)\n",
    "                task_accs.append(acc)\n",
    "            \n",
    "            fast_model.to(torch.device('cpu'))\n",
    "            del fast_model, inner_optimizer\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if training:\n",
    "            # Average gradient across tasks\n",
    "            for i in range(0,len(sum_gradients)):\n",
    "                sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
    "\n",
    "            #Assign gradient for original model, then using optimizer to update its weights\n",
    "            for i, params in enumerate(self.model.parameters()):\n",
    "                params.grad = sum_gradients[i]\n",
    "\n",
    "            self.outer_optimizer.step()\n",
    "            self.outer_optimizer.zero_grad()\n",
    "            \n",
    "            del sum_gradients\n",
    "            gc.collect()\n",
    "        \n",
    "        return np.mean(task_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed(123)\n",
    "test = MetaTask(test_examples, num_task = 3, k_support=80, k_query=20, tokenizer = tokenizer)\n",
    "random_seed(int(time.time() % 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.supports[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "\n",
    "for epoch in range(args.meta_epoch):\n",
    "    \n",
    "    train = MetaTask(train_examples, num_task = 500, k_support=80, k_query=20, tokenizer = tokenizer)\n",
    "    db = create_batch_of_tasks(train, is_shuffle = True, batch_size = args.outer_batch_size)\n",
    "\n",
    "    for step, task_batch in enumerate(db):\n",
    "        \n",
    "        f = open('log.txt', 'a')\n",
    "        \n",
    "        acc = learner(task_batch)\n",
    "        \n",
    "        print('Step:', step, '\\ttraining Acc:', acc)\n",
    "        f.write(str(acc) + '\\n')\n",
    "        \n",
    "        if global_step % 20 == 0:\n",
    "            random_seed(123)\n",
    "            print(\"\\n-----------------Testing Mode-----------------\\n\")\n",
    "            db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
    "            acc_all_test = []\n",
    "\n",
    "            for test_batch in db_test:\n",
    "                acc = learner(test_batch, training = False)\n",
    "                acc_all_test.append(acc)\n",
    "\n",
    "            print('Step:', step, 'Test F1:', np.mean(acc_all_test))\n",
    "            f.write('Test' + str(np.mean(acc_all_test)) + '\\n')\n",
    "            \n",
    "            random_seed(int(time.time() % 10))\n",
    "        \n",
    "        global_step += 1\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "190cdbab7f18599f6194b43e28327b994dce021f407f3b3c3c198d604029e292"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
